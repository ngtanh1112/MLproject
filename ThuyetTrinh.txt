1. Hard Margin vs Softmax (Thực hiện với dữ lieu tuyến tính hoàn toàn)
2. Multi class SVM vs Softmax (Thực hiện với dữ lieu gốc nhiễu)

Đầu tiên sẽ nói về phân loại nhị phân, chọn 2 đặc trưng mạnh nhất là weight và height để phân loại 2 nhãn 1 và 5. Nhìn biểu đồ phân bố dữ lieu ta có thể thấy rang dữ lieu này là phân tách tuyến tính hoàn toàn, ko có nhiễu hay bất kỳ sự chồng chéo nào cả. 

Sẽ dung 2 cách để thực hiện phân loại bài toán này. Cách đầu tiên là SoftMax Regression, cách thứ 2 là Hard Margin SVM. Đối với tất cả các bài toán phân loại dung SoftMax Regression sẽ thực hiện chọn ..... (xem lại),.... Còn về Hard Margin SVM cũng sẽ được lựa chọn vì dữ lieu tách biệt tuyến tính hoàn toàn, Hard Margin có thể dễ dàng thực hiện phân loại.

Bài toán hard margin trong SVM có thể được giải bằng cách chuyển sang bài toán đối ngẫu (dual problem). Việc này giúp dễ dàng hơn khi giải quyết bài toán, đặc biệt khi số lượng các đặc trưng (features) rất lớn so với số lượng các điểm dữ liệu, vì khi chuyển sang bài toán đối ngẫu, chúng ta sẽ làm việc với ma trận nhân chéo giữa các điểm dữ liệu, thay vì làm việc trực tiếp với vector trọng số w.

(Show ảnh chụp kết quả phân loại)

Cả 2 mô hình đều cho ra kết quả đúng, chính xác ngang nhau = 100%. Điều này là dễ hiểu vì dữ liệu input tách biệt tuyến tính hoàn toàn. Mặc dù không đáng kể, nhưng khi thực hiện Softmax Regression trên bộ dữ liệu này thì tốc độ thực thi, huấn luyện của Softmax Regression lại nhanh hơn Hard Margin. Diều này tại vì với dữ liệu đơn giản như trên, Softmax hội tụ nhanh vì gradient được tính dễ dàng và bài toán không phức tạp (Công thức Softmax Regression)
Trong khi đó Hard Margin cần tính toán bài  toán tối ưu hóa có ràng buộc, phải tìm biên tối ưu dẫn đến phải xem xét toàn bộ điểm dữ liệu thay vì chỉ tối ưu trọng số trực tiếp. (Đưa ra công thức tính toán Margin để gợi ý hỏi về cách giải, sẽ giải bằng phương pháp đối ngẫu).

Phần tiếp theo sẽ nói: Tuy nhiên dữ liệu bài toán của chúng ta lại không đơn giản như vậy, dữ liệu sẽ gồm tận (...) chiều và (...) nhãn đầu ra.

(Show ảnh trực quan hóa 2 chiều một phức tạp)

Và còn phức tạp hơn ở chỗ là dữ liệu không tách biệt tuyến tính mà là phi tuyến, với nhiều nhiễu cũng như các điểm ngoại lai. Lúc này sẽ ko sử dụng đc SVM thuần, hay còn gọi là Hard Margin nữa, thay vào đấy ta sẽ có multi class SVM.

2 cách tiếp cận SVM multi class

Sau khi đưa vê bafi toddoon ói ngẫu, sẽ giải bang thư viện CVXOPT tìm ra các nhân tử Lagrange (đại diện cho mức độ quan trọng cho từng iểm dữ lieu). Từ các nhân tử đó (ei), ta tính dc:
Trọng số w = i=1 -> N (ei.xi.yi)
bias = yk - i=1 -> N ei.yi.<xi, xk> (là tích vô hướng, tích lớn nghĩa là vector gần nhau trong không gian đặc trưng, và ngược lại).
bias giúp mô hình tạo ra siêu phẳng phân loại tối ưu mà ko bị giới hạn bởi dữ liệu.
One-vs-Rest (OvR) là một phương pháp mở rộng của SVM để giải quyết bài toán phân loại nhiều lớp (multi-class classification). Phương pháp này chuyển bài toán K lớp thành K  bài toán phân loại nhị phân. sau  tính các giá trị decision function fk(x) = wTk.x +bk. Chọn lớp k có fk(x) lớn nhất làm lớp dự đoán

One-vs-One là Chuyển bài toán K lớp thành K(K-1)/2 bài toán phân loại nhị phân. Mỗi bộ phân loại sẽ dự đoán 1 class cho mỗi điểm dữ liệu, class nào có số phiếu nhiều nhất sẽ là nhãn cho điểm dữ liệu đó. Khi xảy ra trường hợp số phiếu bầu giữa các lớp bằng nhau sẽ tính decicion function và chọn hàm có điểm tin cậy cao nhất làm nhãn dự đoán cho điểm đó




